<!DOCTYPE html>
<html lang=" en-US">

<head>

  
  <meta charset="UTF-8">

  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style"
    type="text/css" crossorigin>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#157878">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
    integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
    integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

  <link rel="stylesheet" href="/stable-pretraining-paradigms-for-llms-workshop.github.io/css/cayman.css?v=">
  <style type="text/css">
    /* Style the tab */
    .tab {
      overflow: hidden;
      border: 1px solid #cccccc;
      background-color: #fdffff;
    }

    /* Style the buttons that are used to open the tab content */
    .tab button {
      background-color: inherit;
      float: left;
      border: none;
      outline: none;
      cursor: pointer;
      padding: 14px 16px;
      transition: 0.3s;
    }

    /* Change background color of buttons on hover */
    .tab button:hover {
      background-color: #dddddd;
    }

    /* Create an active/current tablink class */
    .tab button.active {
      background-color: #cccccc;
    }

    /* Style the tab content */
    .tabcontent {
      display: none;
      padding: 6px 12px;
      border: 1px solid #cccccc;
      border-top: none;
    }
  </style>


</head>

<body>
  <header class="page-header" role="banner">
    <h1 class="project-name">Stable Pre-Training Paradigms for LLMs <br> Reducing Instability, Increasing Capacity</h1>
    <h2 class="project-tagline">CAI Workshop 2025</h2>
    
    <a href="index" class="btn">Workshop</a>
    
    <a href="speakers" class="btn">Speakers</a>
    
    <a href="organizers" class="btn">Organizers</a>
    
    <a href="committee" class="btn">Committee</a>
    
    <a href="schedule" class="btn">Schedule</a>
    
    <a href="question" class="btn">Q&A</a>
    

  </header>

  <main id="content" class="main-content" role="main">
    <p style="text-align: justify;"><b>Our workshop aims to develop stable pre-training paradigms for LLMs that ensure consistent performance and reliability in Large Language Models (LLMs).</b></p>

<h1 id="topic-and-content"><span style="display:block;text-align:center">Topic and Content</span></h1>

<p style="text-align: justify;">
Large Language Models (LLMs) have revolutionized artificial intelligence by achieving remarkable performance across a wide array of tasks. However, the pre-training process of these models often encounters instability issues such as loss spikes, gradient vanishing or exploding, and convergence difficulties. These instabilities not only prolong training time but also affect the overall performance and reliability of the models. As LLMs become increasingly integral to various applications, establishing stable training paradigms is essential. This workshop seeks to bring together researchers and practitioners to discuss and develop strategies for enhancing the stability of LLM pre-training. By focusing on aspects like data quality, optimizer selection, architectural innovations, and spike-awareness mechanisms, we aim to foster collaborations that lead to more robust and dependable LLMs.</p>

<p style="text-align: justify;">
We will cover a range of topics that contribute to the stability of LLM pre-training, including but not limited to:
</p>
<ol>
  <li>
    <b>Data Quality and Preprocessing for Stability</b>
    <p style="text-align: justify;">Investigating how high-quality, well-preprocessed data can enhance training stability. Topics include data cleaning, balancing, augmentation, and the impact of data diversity on preventing overfitting and promoting smooth convergence.</p>
  </li>
  <li>
    <b>Advanced Optimizers for Stable Training</b>
    <p style="text-align: justify;">Exploring optimization algorithms that improve training stability, such as adaptive learning rates, momentum methods, and second-order optimizers. Discussion on how these optimizers can mitigate issues like loss spikes and facilitate consistent gradient flow.</p>
  </li>
    <li>
    <b>Architectural Innovations Promoting Stability</b>
    <p style="text-align: justify;">Examining model architectures that inherently support stable training and preventing vanishing or exploding gradients.</p>
  </li>
    <li>
    <b>Spike-Awareness and Mitigation Techniques</b>
    <p style="text-align: justify;">Developing methods to detect and respond to training instabilities in real-time, specifically focusing on loss spikesâ€”sudden increases during model training. These spikes can signal problems like vanishing or exploding gradients, overfitting, or inappropriate learning rates.</p>
  </li>
    <li>
    <b>Efficient Hardware Utilization for Stability</b>
    <p style="text-align: justify;">Leveraging hardware accelerators and memory management strategies that support stable training of large models. Topics may include gradient checkpointing, mixed-precision training, and the use of specialized hardware to handle extensive computations reliably.</p>
  </li>
    <li>
    <b>Case Studies and Best Practices</b>
    <p style="text-align: justify;">Sharing experiences from successful implementations of stable LLM training, including challenges faced and solutions developed. This includes industry applications where stability was critical for deployment and performance.</p>
  </li>
</ol>

<h1 id="workshop-size"><span style="display:block;text-align:center">Workshop size</span></h1>

<p style="text-align: justify;">
The workshop plans to call for paper submissions and expects around 25 posters with 5 oral presentations among them. The tentative program outline includes interleaved invited speaker sessions and oral presentations, with poster sessions scheduled in the middle of the program. Since stable LLM training has become critically important in the AI community, we are expecting around 50-100 attendees from both academia and industries. We consider a full-day event would suit our workshop.  </p>

<h1 id="diversity-and-inclusion"><span style="display:block;text-align:center">Diversity and Inclusion</span></h1>

<p style="text-align: justify;">Diversity is a core value of our workshop. Our organizing team and invited speakers reflect a broad spectrum of diversity. We have carefully chosen speakers from various <b>affiliations</b> (Max Planck, UNC-Chapel Hill, Google Brain, CMU, and Meta). They represent <b>geographies</b> including the United States and Europe, and their <b>professional backgrounds</b> span both industry and academia. Similarly, our organizers are diverse in <b>gender</b>, with two women and three men, and come from various <b>cultural backgrounds</b>, including Asian and European. Their <b>professional roles</b> range from tech team leaders and postdoctoral researchers to newton fellows and assistant professors.

To encourage the participation of emerging scholars, we are offering a <b>Best Student Paper Award</b> for student submissions, promoting excellence and innovation among junior researchers. We will also provide <b>mentorship</b> programs that pair early-career researchers and participants from underrepresented groups with experienced faculty. We are seeking sponsors to provide <b>travel funding</b> for students, as well as individuals from underrepresented and marginalized groups.</p>


    <footer class="site-footer">
      <hr>
      <!-- 
            <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub
                    Pages</a>.</span> -->
    </footer>
  </main>

</body>

</html>
